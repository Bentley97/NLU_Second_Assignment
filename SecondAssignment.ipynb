{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SecondAssignment.ipynb",
      "provenance": [],
      "mount_file_id": "https://github.com/Bentley97/NLU_Second_Assignment/blob/main/SecondAssignment.ipynb",
      "authorship_tag": "ABX9TyN8cMUP6xFLH/AgtcPYWVvI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bentley97/NLU_Second_Assignment/blob/main/SecondAssignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pB9HUrBUkLor"
      },
      "source": [
        "To clone the repository into the notebook and unzip the conll2003 archive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rg1qsyNBj4O",
        "outputId": "38cc9715-b67f-4ac1-9310-24506242b0cb"
      },
      "source": [
        "%%bash\n",
        "git clone https://github.com/Bentley97/NLU_Second_Assignment.git\n",
        "\n",
        "mkdir NLU_Second_Assignment/src/conll2003\n",
        "\n",
        "unzip -q NLU_Second_Assignment/src/conll2003.zip -d NLU_Second_Assignment/src/conll2003"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'NLU_Second_Assignment'...\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XcMlcT_Seqz"
      },
      "source": [
        "### POST-PROCESS reassamble tokens\n",
        "def reassemble_tokens(doc):\n",
        "  i = 0\n",
        "  j = -1\n",
        "  doc_length = len(doc)\n",
        "  while i != doc_length:\n",
        "    if doc[i].whitespace_ == \"\" and doc[i] != doc[-1]:\n",
        "      if j == -1:\n",
        "        j = i\n",
        "    elif j != -1:\n",
        "      with doc.retokenize() as retokenizer:\n",
        "        retokenizer.merge(doc[j:i+1])\n",
        "      doc_length -= i-j\n",
        "      i = j\n",
        "      j = -1\n",
        "\n",
        "    i += 1\n",
        "\n",
        "  return doc\n",
        "\n",
        "### convert labels from the spacy format to the conll format\n",
        "def convert_labels_into_conll(doc, convert_dict):\n",
        "  temp_hyp = []\n",
        "  for token in doc:\n",
        "    if token.ent_type_ == \"\":\n",
        "      temp_hyp.append((token.text, token.ent_iob_))\n",
        "    else:\n",
        "      temp_hyp.append((token.text, token.ent_iob_+\"-\"+convert_dict[token.ent_type_]))\n",
        "    \n",
        "  return temp_hyp\n",
        "\n",
        "### convert a list of lists of tuples into a list of string(label) with the same order\n",
        "def convert_in_ordered_list_of_label(l):\n",
        "  return [tup[1] for sent in l for tup in sent ]\n",
        "  \n",
        "### builds a list of tuples from a text sentence\n",
        "def build_references(sentence):\n",
        "  return [(e0,e3) for elem in sent for e0,e1,e2,e3 in [elem[0].split(\" \")]]\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Wktq4uLvf1C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9529a68-81b9-449a-99c5-8b7dda0902dc"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.insert(0, os.path.abspath('NLU_Second_Assignment/src'))\n",
        "\n",
        "from conll import read_corpus_conll\n",
        "\n",
        "import spacy\n",
        "from spacy.tokens import Doc\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "trn_url = \"NLU_Second_Assignment/src/conll2003/dev.txt\"\n",
        "trn_url = \"NLU_Second_Assignment/src/conll2003/train.txt\"\n",
        "tst_url = \"NLU_Second_Assignment/src/conll2003/test.txt\"\n",
        "\n",
        "\n",
        "raw_corpus = read_corpus_conll(tst_url) # reading the file\n",
        "raw_corpus.remove(raw_corpus[0])  # remove -DOCSTAR-\n",
        "\n",
        "### loading the english pipeline\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "hyps = []\n",
        "refs = []\n",
        "\n",
        "### cicle over all sentences in the corpus\n",
        "for sent in raw_corpus:\n",
        "  sentence = \" \".join([elem[0].split(\" \")[0] for elem in sent])\n",
        "  \n",
        "  ### building list of references for a sentence and append it to the list of references of the whole\n",
        "  refs.append(build_references(sentence))\n",
        "  \n",
        "  ### call to the NER of spacy\n",
        "  doc = nlp(sentence)\n",
        "\n",
        "  ### POST-PROCESS reassamble tokens\n",
        "  doc = reassemble_tokens(doc)\n",
        "\n",
        "  ### build the list of tuple (text, label) for a sentence converting labels in conll format and appending to list of hypoteses\n",
        "  convert_dict = {\n",
        "      \"PERSON\": \"PER\",\n",
        "      \"ORG\": \"ORG\",\n",
        "      \"LOC\": \"LOC\",\n",
        "      \"GPE\": \"LOC\",\n",
        "      \"FAC\": \"LOC\",\n",
        "      \"CARDINAL\": \"MISC\",\n",
        "      \"DATE\": \"MISC\",\n",
        "      \"EVENT\": \"MISC\",\n",
        "      \"LANGUAGE\": \"MISC\",\n",
        "      \"LAW\": \"MISC\",\n",
        "      \"MONEY\": \"MISC\",\n",
        "      \"NORP\": \"MISC\",\n",
        "      \"ORDINAL\": \"MISC\",\n",
        "      \"PERCENT\": \"MISC\",\n",
        "      \"PRODUCT\": \"MISC\",\n",
        "      \"QUANTITY\": \"MISC\",\n",
        "      \"TIME\": \"MISC\",\n",
        "      \"WORK_OF_ART\": \"MISC\"\n",
        "  }\n",
        "  hyps.append(convert_labels_into_conll(doc, convert_dict))\n",
        "  \n",
        " \n",
        "### adapt hypoteses and references to sklear input format\n",
        "hyps_for_sklearn = convert_in_ordered_list_of_label(hyps)\n",
        "refs_for_sklearn = convert_in_ordered_list_of_label(refs)\n",
        "\n",
        "### extract labels present \n",
        "labels = sorted(list(set(refs_for_sklearn)))\n",
        "\n",
        "\n",
        "### total accuracy is labeld as accuracy\n",
        "print(\"PERFORMANCES token-level:\")\n",
        "print(classification_report(refs_for_sklearn, hyps_for_sklearn, labels=labels, digits=3))\n",
        "\n",
        "print(\"Total accuracy: \",accuracy_score(refs_for_sklearn,hyps_for_sklearn))\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PERFORMANCES token-level:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC      0.786     0.726     0.755      1668\n",
            "      B-MISC      0.091     0.581     0.157       702\n",
            "       B-ORG      0.523     0.337     0.410      1661\n",
            "       B-PER      0.780     0.623     0.693      1617\n",
            "       I-LOC      0.537     0.591     0.563       257\n",
            "      I-MISC      0.055     0.426     0.097       216\n",
            "       I-ORG      0.458     0.550     0.499       835\n",
            "       I-PER      0.736     0.776     0.755      1156\n",
            "           O      0.950     0.840     0.892     38553\n",
            "\n",
            "    accuracy                          0.797     46665\n",
            "   macro avg      0.546     0.606     0.536     46665\n",
            "weighted avg      0.890     0.797     0.836     46665\n",
            "\n",
            "Total accuracy:  0.7966998821386478\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dqy3VmDnH3Wi"
      },
      "source": [
        "**EVALUATE FUNCTION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "FyQKvkDTCOFw",
        "outputId": "c7484f09-b4cc-4d4d-9178-d0bdf98c8df2"
      },
      "source": [
        "from conll import evaluate\n",
        "import pandas as pd\n",
        "\n",
        "results = evaluate(refs, hyps)\n",
        "\n",
        "print(\"PERFORMANCES chunk-level:\")\n",
        "pd_tbl = pd.DataFrame().from_dict(results, orient='index')\n",
        "pd_tbl.round(decimals=3)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PERFORMANCES chunk-level:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>p</th>\n",
              "      <th>r</th>\n",
              "      <th>f</th>\n",
              "      <th>s</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ORG</th>\n",
              "      <td>0.464</td>\n",
              "      <td>0.299</td>\n",
              "      <td>0.363</td>\n",
              "      <td>1661</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PER</th>\n",
              "      <td>0.740</td>\n",
              "      <td>0.592</td>\n",
              "      <td>0.658</td>\n",
              "      <td>1617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LOC</th>\n",
              "      <td>0.777</td>\n",
              "      <td>0.718</td>\n",
              "      <td>0.746</td>\n",
              "      <td>1668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MISC</th>\n",
              "      <td>0.087</td>\n",
              "      <td>0.558</td>\n",
              "      <td>0.151</td>\n",
              "      <td>702</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>total</th>\n",
              "      <td>0.363</td>\n",
              "      <td>0.539</td>\n",
              "      <td>0.433</td>\n",
              "      <td>5648</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           p      r      f     s\n",
              "ORG    0.464  0.299  0.363  1661\n",
              "PER    0.740  0.592  0.658  1617\n",
              "LOC    0.777  0.718  0.746  1668\n",
              "MISC   0.087  0.558  0.151   702\n",
              "total  0.363  0.539  0.433  5648"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkMJOFizkrvD"
      },
      "source": [
        "# **2. Grouping of entities**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jr240bphlmCq"
      },
      "source": [
        "def grouping_entities(doc):\n",
        "#  print(\"ENTITIES:\")\n",
        "#  for ent in doc.ents:\n",
        "#    print(ent, ent[0].idx)\n",
        "#  print(\"ENTITIES!!!\")\n",
        "\n",
        "#  for chunk in doc.noun_chunks:\n",
        "#    print(\"\")\n",
        "#    print(\"CHUNK: \",chunk.text)\n",
        "#    print(chunk, \" | \", chunk.root)\n",
        "#    for ent in chunk.ents:\n",
        "#      print(\"ENT: \", ent.text, \" - \", ent.label_, ent[0].idx)\n",
        "\n",
        "#  print(\"\")\n",
        "#  print(\"\")\n",
        "#  print(\"\")\n",
        "#  print(\"START\")\n",
        "#  print(\"\")\n",
        "\n",
        "  retlist = []\n",
        "  ent_chunked = []\n",
        "\n",
        "#  for ent in doc.ents:\n",
        "    #ent_indexes.append(ent[0].idx)\n",
        "#    print(ent[0].idx)\n",
        "\n",
        "#  print(\"\")\n",
        "#  print(\"FOR\")\n",
        "#  print(\"\")\n",
        "\n",
        "  for ent in doc.ents:\n",
        "#    print(\"\")\n",
        "#    print(ent)\n",
        "    in_chunk = False\n",
        "    if ent[0].idx not in ent_chunked:\n",
        "      for chunk in doc.noun_chunks:\n",
        "        if len(chunk.ents) != 0:\n",
        "#          print(\"CHUNK:: \",chunk, \"  \", chunk.ents)\n",
        "#          print(chunk.ents[0].start_char, chunk.ents[0])\n",
        "          if chunk.ents[0].start_char == ent[0].idx:\n",
        "            in_chunk = True\n",
        "            temp_result = []\n",
        "            for ce in chunk.ents:\n",
        "              temp_result.append(ce.label_)\n",
        "              ent_chunked.append(ce[0].idx)\n",
        "              #result.append([ce.label_ for ce in chunk.ents])\n",
        "            break\n",
        "      if in_chunk == False:\n",
        "        retlist.append([ent.label_])\n",
        "      else:\n",
        "        retlist.append(temp_result)\n",
        "\n",
        "  return retlist\n",
        "  "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpYxuQQVkxfR",
        "outputId": "9fe46250-ca46-4e62-ca8d-c7609d80f98a"
      },
      "source": [
        "import spacy\n",
        "\n",
        "test_sentence = \"Apple's Steve Jobs died in 2011 in Palo Alto , California . Autonomous cars shift insurance liability toward manufacturers in 1996\"\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(test_sentence)\n",
        "groups_of_entities = grouping_entities(doc)\n",
        "print(\"Test grouping function\")\n",
        "print(groups_of_entities)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test grouping function\n",
            "[['ORG', 'PERSON'], ['DATE'], ['GPE'], ['GPE'], ['DATE']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jcK1C1I9wE9"
      },
      "source": [
        "### non tengo conto dell'ordine preché il significato del chunk è diverso (anche se non so che statistica vuole fare il prof)\n",
        "from collections import defaultdict\n",
        "\n",
        "def counting(groups):\n",
        "  dict_group = defaultdict(int)\n",
        "\n",
        "  for g in groups:\n",
        "    key = \", \".join([s for s in g])\n",
        "    dict_group[key] = dict_group[key] + 1\n",
        "\n",
        "  return dict_group\n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mM-Dmgg5NPpX",
        "outputId": "e350c124-7678-464e-904b-9ebfb8624885"
      },
      "source": [
        "import spacy\n",
        "from collections\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "groups = []\n",
        "\n",
        "for sent in raw_corpus:\n",
        "  sentence = \" \".join([elem[0].split(\" \")[0] for elem in sent])\n",
        "  \n",
        "  doc = nlp(sentence)\n",
        "  \n",
        "  groups.extend(grouping_entities(doc))\n",
        "  \n",
        "\n",
        "counts = counting(groups)\n",
        "sort_counts = sorted(counts.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(\"NE groups frequencies:\")\n",
        "for comb in sort_counts:\n",
        "  print(comb)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NE groups frequencies:\n",
            "('CARDINAL', 2116)\n",
            "('GPE', 1346)\n",
            "('DATE', 1140)\n",
            "('PERSON', 1105)\n",
            "('ORG', 955)\n",
            "('NORP', 308)\n",
            "('MONEY', 151)\n",
            "('ORDINAL', 117)\n",
            "('TIME', 92)\n",
            "('PERCENT', 86)\n",
            "('QUANTITY', 82)\n",
            "('EVENT', 58)\n",
            "('LOC', 57)\n",
            "('NORP, PERSON', 47)\n",
            "('CARDINAL, PERSON', 45)\n",
            "('GPE, PERSON', 26)\n",
            "('PRODUCT', 26)\n",
            "('ORG, PERSON', 25)\n",
            "('FAC', 21)\n",
            "('CARDINAL, NORP', 16)\n",
            "('CARDINAL, ORG', 13)\n",
            "('WORK_OF_ART', 12)\n",
            "('GPE, ORG', 11)\n",
            "('GPE, GPE', 11)\n",
            "('CARDINAL, GPE', 11)\n",
            "('PERSON, PERSON', 10)\n",
            "('DATE, EVENT', 9)\n",
            "('ORG, ORG', 9)\n",
            "('LANGUAGE', 8)\n",
            "('LAW', 8)\n",
            "('NORP, ORG', 7)\n",
            "('PERSON, GPE', 6)\n",
            "('DATE, ORG', 6)\n",
            "('GPE, CARDINAL', 5)\n",
            "('DATE, TIME', 5)\n",
            "('DATE, NORP', 5)\n",
            "('ORG, GPE', 4)\n",
            "('CARDINAL, CARDINAL', 4)\n",
            "('NORP, NORP', 4)\n",
            "('GPE, NORP', 4)\n",
            "('ORG, DATE', 4)\n",
            "('CARDINAL, DATE', 3)\n",
            "('ORDINAL, PERSON', 3)\n",
            "('GPE, ORDINAL', 3)\n",
            "('NORP, GPE', 3)\n",
            "('ORDINAL, CARDINAL', 2)\n",
            "('DATE, PRODUCT', 2)\n",
            "('NORP, DATE', 2)\n",
            "('ORG, NORP', 2)\n",
            "('MONEY, ORG', 2)\n",
            "('PERSON, ORG', 2)\n",
            "('ORG, LOC', 2)\n",
            "('DATE, PERSON', 2)\n",
            "('ORDINAL, EVENT', 2)\n",
            "('GPE, FAC', 2)\n",
            "('GPE, DATE', 2)\n",
            "('GPE, LOC', 2)\n",
            "('ORG, PRODUCT', 2)\n",
            "('CARDINAL, LOC', 2)\n",
            "('PERSON, GPE, CARDINAL', 2)\n",
            "('EVENT, CARDINAL', 2)\n",
            "('DATE, CARDINAL', 2)\n",
            "('ORDINAL, NORP', 1)\n",
            "('PERSON, CARDINAL', 1)\n",
            "('ORDINAL, DATE', 1)\n",
            "('PERSON, PERSON, ORG', 1)\n",
            "('ORDINAL, PRODUCT', 1)\n",
            "('CARDINAL, PRODUCT', 1)\n",
            "('PERSON, ORDINAL', 1)\n",
            "('GPE, CARDINAL, GPE', 1)\n",
            "('ORDINAL, ORG', 1)\n",
            "('CARDINAL, PERSON, GPE', 1)\n",
            "('NORP, ORDINAL', 1)\n",
            "('MONEY, EVENT', 1)\n",
            "('CARDINAL, GPE, CARDINAL', 1)\n",
            "('PRODUCT, ORDINAL, NORP', 1)\n",
            "('NORP, PRODUCT', 1)\n",
            "('NORP, NORP, PERSON', 1)\n",
            "('ORG, PERSON, ORG', 1)\n",
            "('LOC, DATE', 1)\n",
            "('NORP, ORDINAL, WORK_OF_ART', 1)\n",
            "('GPE, CARDINAL, NORP', 1)\n",
            "('NORP, ORG, DATE', 1)\n",
            "('LOC, ORDINAL', 1)\n",
            "('MONEY, GPE', 1)\n",
            "('CARDINAL, CARDINAL, ORG', 1)\n",
            "('CARDINAL, PERSON, ORG', 1)\n",
            "('ORDINAL, GPE', 1)\n",
            "('ORG, GPE, LOC, LOC', 1)\n",
            "('MONEY, DATE', 1)\n",
            "('NORP, ORG, QUANTITY', 1)\n",
            "('GPE, ORG, PERSON', 1)\n",
            "('DATE, WORK_OF_ART', 1)\n",
            "('CARDINAL, TIME', 1)\n",
            "('MONEY, MONEY', 1)\n",
            "('PRODUCT, ORG', 1)\n",
            "('ORG, PERCENT, MONEY, CARDINAL, ORG', 1)\n",
            "('PERSON, EVENT', 1)\n",
            "('LOC, NORP', 1)\n",
            "('ORG, WORK_OF_ART', 1)\n",
            "('GPE, ORDINAL, PERSON', 1)\n",
            "('DATE, GPE', 1)\n",
            "('ORG, CARDINAL', 1)\n",
            "('ORG, CARDINAL, ORG', 1)\n",
            "('PRODUCT, PRODUCT', 1)\n",
            "('ORDINAL, ORDINAL', 1)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}